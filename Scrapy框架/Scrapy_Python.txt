————————————————————————————————————————————————————————————————————————————————————————————————————————
关于Scrapy：
是目前应用最广，应用最频繁的一个python库
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。
简单来说就是爬取框架性内容的库
————————————————————————————————————————————————————————————————————————————————————————————————————————
其他的库都是直接引用
但是scrapy要在终端里创建一个新的文档包，有很多文件
    格式为：scrapy startproject 文件夹的名字

创建爬虫文件：
    scrapy比价特殊，要在终端中创建一个爬虫文件
    格式为：scrapy genspider 爬虫文件的名字 要爬取的网页


运行爬虫文件：
     scrapy crawl 爬虫的名字
     eg:
     scrapy crawl baidu

百度或其他网站会有君子协议：
robots协议，可以在scrapy中的setting中停止对于robots协议的遵守
之后就可以成功运行scrapy脚本了
————————————————————————————————————————————————————————————————————————————————————————————————————————

scrapy 项目结构
    项目名称
        项目名字
            __pycache__文件夹：
            （更新之后产生的文件，不知道是干什么的）
            spiders文件夹（存储的是爬虫文件）
                init（暂时不用管）
                自定义的爬虫文件  （scrapy 的核心）

            init（暂时不用管）
            items           定义数据结构的地方，爬取的数据包括什么
            middlewares     中间件 主要应用于代理
            pipelines       管道 用来处理下载的数据
            setting         配置文件 robots协议 ua定义等等

————————————————————————————————————————————————————————————————————————————————————————————————————————

response 语法结构
    response.text   获取相应的字符串
    response.body   获取二进制数据
    response.xpath  可以直接使用xpath方法来解析response中的内容
    res_List.extract()  可以提取seletor对象的data属性值
    
————————————————————————————————————————————————————————————————————————————————————————————————————————

*******************
Scrapy的工作原理：
    1、引擎向spiders索要url
    2、引擎将要爬取的url传给调度器
    3、调度器会将url生成请求对象放入到指定的队列中
    4、从队列中出队一个请求
    5、引擎将请求交给下载器进行处理
    6、下载器发送请求获取互联网数据
    7、下载器将数据返回给引擎
    8、引擎将数据再次给到spiders
    9、spiders通过xpath解析该数据，得到数据或者url
    10、spiders将数据或者url给到引擎
    11、引擎判断该数据是成分，若是数据，交给管道（item piplines）处理；是url则交给调度器处理
*******************

————————————————————————————————————————————————————————————————————————————————————————————————————————

链接提取器：
首先导入链接提取器的包
from scrapy.linkextractors import linkextractors

链接提取器本质作用就是可以提取相同类型的链接
主要的方法有
使用allow
1.linkExtractors(allow = r'不同链接的相同部分/……/……\d+代表一个或者多个数字\.html仍然是相同部分')
使用xpath：注意，这里要用xpaths
2.linkExtractors(restract_xpaths = '//xpath的路径')

显示爬取的路径：
link.extract_links(response)
注意：爬取的路径会被覆盖

————————————————————————————————————————————————————————————————————————————————————————————————————————
日志级别与日志信息：

    日志级别：
        CRITICAL:严重错误
        ERROR:一般错误
        WARNING:警告
        INFO:一般信息
        DEBUG:调试信息
    在setting中：
    创建一个
    LOG_FILE:将屏幕和显示的信息记录到文件中，屏幕上将不会显示，文件后缀一定要是.log
    LOG_LEVEL:设置日志等级；相当于是向显示什么不想显示什么，都可以在这里设置，想显示的就直接打上

————————————————————————————————————————————————————————————————————————————————————————————————————————
